{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 資料處理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a. Bitscore 標準建立"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a-1. Actinobacteria Negative Control 之 Bitscore 標準"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HMM actino negative control bit-score threshold\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "# create a function can get the datafarme of each control's best bit-score, e-vale and coverage dataframes\n",
    "# hmm domtblout name should be ...aed_I_... NOT ..._I_aed_....\n",
    "def negative_control_df(control_dir, control_names, BitScore_df):\n",
    "    # Create an empty dictionary to store the best bit-scores; best e-vale; best coverage for each hmmsearch\n",
    "    best_bit_scores = {}\n",
    "\n",
    "    # Loop over the hmmsearches and parse the corresponding \"domtblout\" file\n",
    "    for control_name in control_names:\n",
    "        # Load the \"domtblout\" file into a pandas DataFrame\n",
    "        file_path = os.path.join(control_dir, control_name + \".domtblout\")\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, comment=\"#\", sep='\\s+', header=None)\n",
    "        except pd.errors.EmptyDataError:\n",
    "            df = pd.DataFrame()\n",
    "\n",
    "        if df.empty:\n",
    "            pattern = r'A_.*_aed'\n",
    "            query_name = re.sub(pattern, 'A_aed', control_name)        \n",
    "            best_bit_scores[query_name] = 0\n",
    "        else:        \n",
    "            # Assign column names to the DataFrame\n",
    "            df.columns = [\"target_name\", \"accession\", \"tlen\", \"query_name\", \"accession2\", \"qlen\", \"E-value\", \"score\", \"bias\",\n",
    "                          \"num_domains_index\", \"num_domains_total\", \"c-Evalue\", \"i-Evalue\", \"score2\", \"bias2\", \"hmm_from\", \"hmm_to\", \"ali_from\", \"ali_to\",\n",
    "                          \"env_from\", \"env_to\", \"acc\", \"description\"]\n",
    "            # Calculate the coverage for each hit\n",
    "            df[\"coverage\"] = (df[\"ali_to\"] - df[\"ali_from\"] + 1) / df[\"tlen\"]\n",
    "\n",
    "            # Filter the DataFrame by E-value and coverage, and sort by bit-score\n",
    "            significant_hits = df[(df[\"E-value\"] < 0.001) & (df[\"coverage\"] > 0.50)].sort_values(by=\"score\", ascending=False)\n",
    "\n",
    "            # replace strains name to aed..\n",
    "            pattern = r'A_.*_aed'\n",
    "            query_name = re.sub(pattern, 'A_aed', control_name)\n",
    "\n",
    "            # Extract the best bit-score and store it in the dictionary\n",
    "            if not significant_hits.empty:\n",
    "                best_bit_score = significant_hits.iloc[0][\"score\"]\n",
    "                best_bit_scores[query_name] = best_bit_score\n",
    "            else:\n",
    "                best_bit_scores[query_name] = 0\n",
    "\n",
    "    # create the index names for three df\n",
    "    # Get the strain name\n",
    "    pattern = r'A_.*_aed'\n",
    "    StrainName = re.findall(pattern, control_names[0])\n",
    "    \n",
    "    # create index of bit score\n",
    "    bitscore_name = StrainName[0] + '_bit_score'\n",
    "    BitScore_Name = [bitscore_name]\n",
    "\n",
    "    # create a dataframe of bit score\n",
    "    BitScore_df = pd.DataFrame(best_bit_scores, index=BitScore_Name)\n",
    "\n",
    "    return BitScore_df\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------\n",
    "# A_Rhodococcus_jostii_RHA1\n",
    "# Define the directory that contains the \"domtblout\" files\n",
    "control_dir = \"../data/raw/Actino_HMM_Control/Negative/A_Rhodococcus_jostii_RHA1/domtblout/\"\n",
    "\n",
    "# Define the names of the control_names (without the file extension)\n",
    "# Do not use 'A_aedC_RS26365', 'A_aedQ_RS26445', 'A_aedR_RS26450' cause their Non-All homologous\n",
    "control_names = ['A_Rhodococcus_jostii_RHA1_aedA_I_RS26385', 'A_Rhodococcus_jostii_RHA1_aedB_I_RS26395', 'A_Rhodococcus_jostii_RHA1_aedD_RS26370', 'A_Rhodococcus_jostii_RHA1_aedE_RS26375', 'A_Rhodococcus_jostii_RHA1_aedF_I_RS26380', 'A_Rhodococcus_jostii_RHA1_aedG_I_RS26390', 'A_Rhodococcus_jostii_RHA1_aedH_I_RS26400', 'A_Rhodococcus_jostii_RHA1_aedI_RS26405', 'A_Rhodococcus_jostii_RHA1_aedJ_I_RS26410', 'A_Rhodococcus_jostii_RHA1_aedK_I_RS26415', 'A_Rhodococcus_jostii_RHA1_aedL_RS26420', 'A_Rhodococcus_jostii_RHA1_aedM_RS26425', 'A_Rhodococcus_jostii_RHA1_aedN_RS26430', 'A_Rhodococcus_jostii_RHA1_aedO_RS26435', 'A_Rhodococcus_jostii_RHA1_aedP_RS26440']\n",
    "\n",
    "# Use positive_control_df function to get the df (control_dir, control_names, BitScore_df, Evalue_df, Coverage_df)\n",
    "RHA1_BitScore = pd.DataFrame()\n",
    "RHA1_BitScore = negative_control_df(control_dir, control_names, RHA1_BitScore)\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------\n",
    "# A_Mycobacterium_tuberculosis_H37Rv\n",
    "# Define the directory that contains the \"domtblout\" files\n",
    "control_dir = \"../data/raw/Actino_HMM_Control/Negative/A_Mycobacterium_tuberculosis_H37Rv/domtblout\"\n",
    "\n",
    "# Define the names of the control_names (without the file extension)\n",
    "# Do not use 'A_aedC_RS26365', 'A_aedQ_RS26445', 'A_aedR_RS26450' cause their Non-All homologous\n",
    "control_names = ['A_Mycobacterium_tuberculosis_H37Rv_aedA_I_RS26385', 'A_Mycobacterium_tuberculosis_H37Rv_aedB_I_RS26395', 'A_Mycobacterium_tuberculosis_H37Rv_aedD_RS26370', 'A_Mycobacterium_tuberculosis_H37Rv_aedE_RS26375', 'A_Mycobacterium_tuberculosis_H37Rv_aedF_I_RS26380', 'A_Mycobacterium_tuberculosis_H37Rv_aedG_I_RS26390', 'A_Mycobacterium_tuberculosis_H37Rv_aedH_I_RS26400', 'A_Mycobacterium_tuberculosis_H37Rv_aedI_RS26405', 'A_Mycobacterium_tuberculosis_H37Rv_aedJ_I_RS26410', 'A_Mycobacterium_tuberculosis_H37Rv_aedK_I_RS26415', 'A_Mycobacterium_tuberculosis_H37Rv_aedL_RS26420', 'A_Mycobacterium_tuberculosis_H37Rv_aedM_RS26425', 'A_Mycobacterium_tuberculosis_H37Rv_aedN_RS26430', 'A_Mycobacterium_tuberculosis_H37Rv_aedO_RS26435', 'A_Mycobacterium_tuberculosis_H37Rv_aedP_RS26440']\n",
    "\n",
    "# Use positive_control_df function to get the df (control_dir, control_names, BitScore_df, Evalue_df, Coverage_df)\n",
    "H37Rv_BitScore = pd.DataFrame()\n",
    "H37Rv_BitScore = negative_control_df(control_dir, control_names, H37Rv_BitScore)\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "# merge two negative best bit score df\n",
    "Actino_Negative_df = pd.concat([RHA1_BitScore, H37Rv_BitScore], axis=0)\n",
    "\n",
    "# Got the highest bit score of the df\n",
    "Actino_BitScore_Criteria = Actino_Negative_df.max()\n",
    "Actino_BitScore_Criteria = Actino_BitScore_Criteria.iloc[0:]\n",
    "Actino_BitScore_Criteria = Actino_BitScore_Criteria.to_frame()\n",
    "Actino_BitScore_Criteria.columns = ['Criteria_Bitscore']\n",
    "\n",
    "# transpose the DataFrame\n",
    "Actino_BitScore_Criteria_T = Actino_BitScore_Criteria.T\n",
    "\n",
    "# merge all nad Min table\n",
    "Actino_Negative_df = pd.concat([Actino_Negative_df, Actino_BitScore_Criteria_T], axis=0)\n",
    "Actino_Negative_df.to_csv('../data/processed/Final/ForReader/ControlData/Actino_aed/Actino_NegativeBitscore.csv')\n",
    "\n",
    "# done\n",
    "display(Actino_BitScore_Criteria)\n",
    "display(Actino_Negative_df)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a-2 Proteobcateria Negative Control 之 Bitscore 建立"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HMM actino negative control bit-score threshold\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "# create a function can get the datafarme of each control's best bit-score, e-vale and coverage dataframes\n",
    "# hmm domtblout name should be ...edc_I_... NOT ..._I_edc_....\n",
    "def negative_control_df(control_dir, control_names, BitScore_df):\n",
    "    # Create an empty dictionary to store the best bit-scores; best e-vale; best coverage for each hmmsearch\n",
    "    best_bit_scores = {}\n",
    "\n",
    "    # Loop over the hmmsearches and parse the corresponding \"domtblout\" file\n",
    "    for control_name in control_names:\n",
    "        # Load the \"domtblout\" file into a pandas DataFrame\n",
    "        file_path = os.path.join(control_dir, control_name + \".domtblout\")\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, comment=\"#\", sep='\\s+', header=None)\n",
    "        except pd.errors.EmptyDataError:\n",
    "            df = pd.DataFrame()\n",
    "\n",
    "        if df.empty:\n",
    "            pattern = r'P_.*_edc'\n",
    "            query_name = re.sub(pattern, 'P_edc', control_name)         \n",
    "            best_bit_scores[query_name] = 0\n",
    "        else:        \n",
    "            # Assign column names to the DataFrame\n",
    "            df.columns = [\"target_name\", \"accession\", \"tlen\", \"query_name\", \"accession2\", \"qlen\", \"E-value\", \"score\", \"bias\",\n",
    "                          \"num_domains_index\", \"num_domains_total\", \"c-Evalue\", \"i-Evalue\", \"score2\", \"bias2\", \"hmm_from\", \"hmm_to\", \"ali_from\", \"ali_to\",\n",
    "                          \"env_from\", \"env_to\", \"acc\", \"description\"]\n",
    "            # Calculate the coverage for each hit\n",
    "            df[\"coverage\"] = (df[\"ali_to\"] - df[\"ali_from\"] + 1) / df[\"tlen\"]\n",
    "\n",
    "            # Filter the DataFrame by E-value and coverage, and sort by bit-score\n",
    "            significant_hits = df[(df[\"E-value\"] < 0.001) & (df[\"coverage\"] > 0.50)].sort_values(by=\"score\", ascending=False)\n",
    "\n",
    "            # replace strains name to aed..\n",
    "            pattern = r'P_.*_edc'\n",
    "            query_name = re.sub(pattern, 'P_edc', control_name)  \n",
    "\n",
    "            # Extract the best bit-score and store it in the dictionary\n",
    "            if not significant_hits.empty:\n",
    "                best_bit_score = significant_hits.iloc[0][\"score\"]\n",
    "                best_bit_scores[query_name] = best_bit_score\n",
    "            else:\n",
    "                best_bit_scores[query_name] = 0\n",
    "\n",
    "    # create the index names for three df\n",
    "    # Get the strain name\n",
    "    pattern = r'P_.*_edc'\n",
    "    StrainName = re.findall(pattern, control_names[0])\n",
    "\n",
    "    # create index of bit score\n",
    "    bitscore_name = StrainName[0] + '_bit_score'\n",
    "    BitScore_Name = [bitscore_name]\n",
    "\n",
    "    # create a dataframe of bit score\n",
    "    BitScore_df = pd.DataFrame(best_bit_scores, index=BitScore_Name)\n",
    "\n",
    "    return BitScore_df\n",
    "\n",
    "\n",
    "#---------------------------\n",
    "# P_Comamonas_thiooxidans_CNB1\n",
    "# Define the directory that contains the \"domtblout\" files, 這個genome protein fasta要把discription的文字刪除，因有空格會影響read\n",
    "control_dir = \"../data/raw/Proteo_HMM_NegativeControl/P_Comamonas_thiooxidans_CNB1/\"\n",
    "\n",
    "# Define the names of the control_names (without the file extension); delet 13540\n",
    "control_names = ['P_Comamonas_thiooxidans_CNB1_edc13525_I_', 'P_Comamonas_thiooxidans_CNB1_edc13530', 'P_Comamonas_thiooxidans_CNB1_edc13535', 'P_Comamonas_thiooxidans_CNB1_edc13545', 'P_Comamonas_thiooxidans_CNB1_edc13550', 'P_Comamonas_thiooxidans_CNB1_edc13555', 'P_Comamonas_thiooxidans_CNB1_edc13560', 'P_Comamonas_thiooxidans_CNB1_edc13565', 'P_Comamonas_thiooxidans_CNB1_edc13570_I_', 'P_Comamonas_thiooxidans_CNB1_edc13575', 'P_Comamonas_thiooxidans_CNB1_edc13580_I_', 'P_Comamonas_thiooxidans_CNB1_edc13585', 'P_Comamonas_thiooxidans_CNB1_edc13590', 'P_Comamonas_thiooxidans_CNB1_edc13595']\n",
    "\n",
    "# Use positive_control_df function to get the df (control_dir, control_names, BitScore_df)\n",
    "CNB1_BitScore = pd.DataFrame()\n",
    "CNB1_BitScore = negative_control_df(control_dir, control_names, CNB1_BitScore)\n",
    "\n",
    "\n",
    "#---------------------------\n",
    "# P_Novosphingobium_sp_strain_Chol11\n",
    "# Define the directory that contains the \"domtblout\" files, 這個genome protein fasta要把discription的文字刪除，因有空格會影響read\n",
    "control_dir = \"../data/raw/Proteo_HMM_NegativeControl/P_Novosphingobium_sp_strain_Chol11/\"\n",
    "\n",
    "# Define the names of the control_names (without the file extension); delet 13540\n",
    "control_names = ['P_Novosphingobium_sp_strain_Chol11_edc13525_I_', 'P_Novosphingobium_sp_strain_Chol11_edc13530', 'P_Novosphingobium_sp_strain_Chol11_edc13535', 'P_Novosphingobium_sp_strain_Chol11_edc13545', 'P_Novosphingobium_sp_strain_Chol11_edc13550', 'P_Novosphingobium_sp_strain_Chol11_edc13555', 'P_Novosphingobium_sp_strain_Chol11_edc13560', 'P_Novosphingobium_sp_strain_Chol11_edc13565', 'P_Novosphingobium_sp_strain_Chol11_edc13570_I_', 'P_Novosphingobium_sp_strain_Chol11_edc13575', 'P_Novosphingobium_sp_strain_Chol11_edc13580_I_', 'P_Novosphingobium_sp_strain_Chol11_edc13585', 'P_Novosphingobium_sp_strain_Chol11_edc13590', 'P_Novosphingobium_sp_strain_Chol11_edc13595']\n",
    "\n",
    "# Use positive_control_df function to get the df (control_dir, control_names, BitScore_df)\n",
    "Chol11_BitScore = pd.DataFrame()\n",
    "Chol11_BitScore = negative_control_df(control_dir, control_names, Chol11_BitScore)\n",
    "\n",
    "\n",
    "#---------------------------\n",
    "# P_Pseudomonas_putida_DOC21_cluster\n",
    "# Define the directory that contains the \"domtblout\" files, 這個genome protein fasta要把discription的文字刪除，因有空格會影響read\n",
    "control_dir = \"../data/raw/Proteo_HMM_NegativeControl/P_Pseudomonas_putida_DOC21_cluster/\"\n",
    "\n",
    "# Define the names of the control_names (without the file extension); delet 13540\n",
    "control_names = ['P_Pseudomonas_putida_DOC21_cluster1_edc13525_I_', 'P_Pseudomonas_putida_DOC21_cluster1_edc13530', 'P_Pseudomonas_putida_DOC21_cluster1_edc13535', 'P_Pseudomonas_putida_DOC21_cluster1_edc13545', 'P_Pseudomonas_putida_DOC21_cluster1_edc13550', 'P_Pseudomonas_putida_DOC21_cluster1_edc13555', 'P_Pseudomonas_putida_DOC21_cluster1_edc13560', 'P_Pseudomonas_putida_DOC21_cluster1_edc13565', 'P_Pseudomonas_putida_DOC21_cluster1_edc13570_I_', 'P_Pseudomonas_putida_DOC21_cluster1_edc13575', 'P_Pseudomonas_putida_DOC21_cluster1_edc13580_I_', 'P_Pseudomonas_putida_DOC21_cluster1_edc13585', 'P_Pseudomonas_putida_DOC21_cluster1_edc13590', 'P_Pseudomonas_putida_DOC21_cluster1_edc13595']\n",
    "\n",
    "# Use positive_control_df function to get the df (control_dir, control_names, BitScore_df)\n",
    "DOC21_BitScore = pd.DataFrame()\n",
    "DOC21_BitScore = negative_control_df(control_dir, control_names, DOC21_BitScore)\n",
    "\n",
    "\n",
    "#---------------------------\n",
    "# P_Pseudomonas_stutzeri_Chol-1\n",
    "# Define the directory that contains the \"domtblout\" files, 這個genome protein fasta要把discription的文字刪除，因有空格會影響read\n",
    "control_dir = \"../data/raw/Proteo_HMM_NegativeControl/P_Pseudomonas_stutzeri_Chol-1/\"\n",
    "\n",
    "# Define the names of the control_names (without the file extension); delet 13540\n",
    "control_names = ['P_Pseudomonas_stutzeri_Chol-1_edc13525_I_', 'P_Pseudomonas_stutzeri_Chol-1_edc13530', 'P_Pseudomonas_stutzeri_Chol-1_edc13535', 'P_Pseudomonas_stutzeri_Chol-1_edc13545', 'P_Pseudomonas_stutzeri_Chol-1_edc13550', 'P_Pseudomonas_stutzeri_Chol-1_edc13555', 'P_Pseudomonas_stutzeri_Chol-1_edc13560', 'P_Pseudomonas_stutzeri_Chol-1_edc13565', 'P_Pseudomonas_stutzeri_Chol-1_edc13570_I_', 'P_Pseudomonas_stutzeri_Chol-1_edc13575', 'P_Pseudomonas_stutzeri_Chol-1_edc13580_I_', 'P_Pseudomonas_stutzeri_Chol-1_edc13585', 'P_Pseudomonas_stutzeri_Chol-1_edc13590', 'P_Pseudomonas_stutzeri_Chol-1_edc13595']\n",
    "\n",
    "# Use positive_control_df function to get the df (control_dir, control_names, BitScore_df)\n",
    "Chol01_BitScore = pd.DataFrame()\n",
    "Chol01_BitScore = negative_control_df(control_dir, control_names, Chol01_BitScore)\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "# merge four negative best bit score df\n",
    "Proteo_Negative_df = pd.concat([CNB1_BitScore, Chol11_BitScore, DOC21_BitScore, Chol01_BitScore], axis=0)\n",
    "\n",
    "# Got the highest bit score of the df\n",
    "Proteo_BitScore_Criteria = Proteo_Negative_df.max()\n",
    "Proteo_BitScore_Criteria = Proteo_BitScore_Criteria.iloc[0:]\n",
    "Proteo_BitScore_Criteria = Proteo_BitScore_Criteria.to_frame()\n",
    "Proteo_BitScore_Criteria.columns = ['Criteria_Bitscore']\n",
    "\n",
    "# transpose the DataFrame\n",
    "Proteo_BitScore_Criteria_T = Proteo_BitScore_Criteria.T\n",
    "\n",
    "# merge all nad Min table\n",
    "Proteo_Negative_df = pd.concat([Proteo_Negative_df, Proteo_BitScore_Criteria_T], axis=0)\n",
    "Proteo_Negative_df.to_csv('../data/processed/Final/ForReader/ControlData/Proteo_edc/Proteo_NegativeBitscore.csv')\n",
    "\n",
    "# done\n",
    "display(Proteo_BitScore_Criteria)\n",
    "display(Proteo_Negative_df)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b. 利用上述生成的標準來篩選 MAGs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b-1. Actinobacteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1b-1a 在 HMMER 分析中用 Bitscore 標準來篩選出相似的 Protein of MAGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actino_HMM_MAGs\n",
    "#需先進行前步驟的cell (1. 使用 negative control 來獲得合適的bit score)\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "#-------------------------------\n",
    "#Aed Cluster to MAGs\n",
    "\n",
    "#Define the directory that contains the \"domtblout\" files.需要刪除discription\n",
    "domtblout_dir = \"../data/raw/Actino_HMM_MAGs_domtblout/\"\n",
    "\n",
    "#Create an empty dictionary to store the target name for each hmmsearch\n",
    "MAGs_Hits = {} #create a dictionary\n",
    "MAGs_Hits_name = [] #創建 List，存入篩選到的 MAGs ID\n",
    "\n",
    "#covert criteria dataframe to serires\n",
    "Actino_BitScore_Criteria_S = Actino_BitScore_Criteria['Criteria_Bitscore']\n",
    "Actino_BitScore_Criteria_S = Actino_BitScore_Criteria_S.astype(float)    \n",
    "\n",
    "#create a dataframe for a all hits \n",
    "columns = [\"target_name\", \"accession\", \"tlen\", \"query_name\", \"accession2\", \"qlen\", \"E-value\", \"score\", \"bias\",\n",
    "           \"num_domains_index\", \"num_domains_total\", \"c-Evalue\", \"i-Evalue\", \"score2\", \"bias2\", \"hmm_from\",\n",
    "           \"hmm_to\", \"ali_from\", \"ali_to\", \"env_from\", \"env_to\", \"acc\", \"description\"]\n",
    "\n",
    "All_aed_Hits_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "#Loop over the HMM DOMTBLOUT files and filter the results based on bit score, e-value and coverage\n",
    "#hmm name and bit score are in the Actino_BitScore_Criteria series\n",
    "for hmmsearch, threshold in Actino_BitScore_Criteria_S.items():\n",
    "    #Load the \"domtblout\" file into a pandas DataFrame\n",
    "    file_path = os.path.join(domtblout_dir, hmmsearch + \".domtblout\")\n",
    "    df = pd.read_csv(file_path, comment=\"#\", sep='\\s+', header=None)    \n",
    "    #Assign column names to the DataFrame\n",
    "    df.columns = [\"target_name\", \"accession\", \"tlen\", \"query_name\", \"accession2\", \"qlen\", \"E-value\", \"score\", \"bias\",\n",
    "                  \"num_domains_index\", \"num_domains_total\", \"c-Evalue\", \"i-Evalue\", \"score2\", \"bias2\", \"hmm_from\", \"hmm_to\", \"ali_from\", \"ali_to\",\n",
    "                  \"env_from\", \"env_to\", \"acc\", \"description\"]\n",
    "\n",
    "    #Calculate the coverage for each hit\n",
    "    df[\"coverage\"] = (df[\"ali_to\"] - df[\"ali_from\"] + 1) / df[\"tlen\"]\n",
    "    \n",
    "    #Filter the DataFrame by E-value, coverage, and bit-score\n",
    "    significant_hits = df[(df[\"E-value\"] < 0.001) & (df[\"coverage\"] > 0.50) & (df[\"score\"] > threshold)]\n",
    "\n",
    "    #Extract Target nmae and store it in the dictionary\n",
    "    if not significant_hits.empty:\n",
    "        MAGs_Hits_name = significant_hits[\"target_name\"].tolist()\n",
    "        MAGs_Hits[hmmsearch] = MAGs_Hits_name\n",
    "    else:\n",
    "        MAGs_Hits_name = None\n",
    "        MAGs_Hits[hmmsearch] = MAGs_Hits_name\n",
    "    \n",
    "    #add hits table to a df\n",
    "    All_aed_Hits_df = pd.concat([significant_hits, All_aed_Hits_df], axis=0)\n",
    "\n",
    "#done\n",
    "print('done')\n",
    "print('unique query name/numbers: ', All_aed_Hits_df['query_name'].unique(), ' / ', len(All_aed_Hits_df['query_name'].unique()))\n",
    "All_aed_Hits_df.to_csv('../data/processed/All_aed_Hits_df_bitscore.csv')    \n",
    "All_aed_Hits_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1b-1b 整理出各個 MAGs 所具有的 Protein Hits，再用hits數量進行篩選(>=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Got target name and query name\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "#read table\n",
    "All_aed_Hits_df = pd.read_csv('../data/processed/All_aed_Hits_df_bitscore.csv') \n",
    "\n",
    "#Got target name and query name    \n",
    "All_aed_Hits_TargetAndQuery = All_aed_Hits_df[['target_name', 'query_name']]\n",
    "\n",
    "#Load a Dataframe with the lookup values for merge protein id to MAGs and merge them\n",
    "TarToMAGs_aed = pd.read_csv('../data/interim/Actino_aed/aed_All_TarToMAGsID.csv')\n",
    "\n",
    "#use merge() function to join the MAGsID data\n",
    "aed_hits_TargetAndMAGsID = pd.merge(All_aed_Hits_TargetAndQuery, TarToMAGs_aed, on='target_name', how='left')                                                                               \n",
    "\n",
    "#check the null value\n",
    "print('Any Null in TargetToMAGsID: ', aed_hits_TargetAndMAGsID['MAGsID'].isnull().any())\n",
    "\n",
    "#create the crosstab table (like heatmap)\n",
    "aed_hits_heatmap = pd.crosstab(aed_hits_TargetAndMAGsID['query_name'], aed_hits_TargetAndMAGsID['MAGsID'], dropna=False)\n",
    "aed_hits_heatmap = aed_hits_heatmap.transpose()\n",
    "#aed_hits_heatmap\n",
    "#aed_hits_heatmap.to_csv('../data/processed/aed_hits_heatmap_bitscore.csv')\n",
    "\n",
    "#Count the non-zero values in hmm profiles hit row to calculate the number of different HMM profiles that have hits in a given MAG. \n",
    "def count_nonzero(row):\n",
    "    return len(row[row != 0])\n",
    "\n",
    "num_hits = aed_hits_heatmap.apply(count_nonzero, axis=1)\n",
    "aed_hits_heatmap['num_hits'] = num_hits\n",
    "\n",
    "#sort them by hits numer\n",
    "aed_hits_heatmap = aed_hits_heatmap.sort_values(by=\"num_hits\", ascending=False)\n",
    "\n",
    "#extract > 8 hmm profiles hits and the necessary hits (aedA、aedB、aedJ)\n",
    "aed_hits_FinalFilter =  aed_hits_heatmap[(aed_hits_heatmap['num_hits'] >= 8)]\n",
    "#>7 = 326, >8 = 167, >9 = 70, >10 = 17 用8較為恰當 大於一半的query gene\n",
    "\n",
    "#Reset index and move index column to first position\n",
    "aed_hits_FinalFilter.index.name = None\n",
    "aed_hits_FinalFilter = aed_hits_FinalFilter.reset_index()\n",
    "aed_hits_FinalFilter.insert(0, 'index', aed_hits_FinalFilter.pop('index'))\n",
    "\n",
    "#rename MAGsID\n",
    "aed_hits_FinalFilter = aed_hits_FinalFilter.rename(columns={'index': 'genome_id'})\n",
    "\n",
    "#extract the MAGsID and num_hits column\n",
    "aed_Positive_MAGsID = aed_hits_FinalFilter[['genome_id', 'num_hits']]\n",
    "\n",
    "#done\n",
    "print('done')\n",
    "aed_Positive_MAGsID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1b-1c 將篩選出的 MAGS 與相關的資料彙整至 reference 的 MAGs metadata 中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#需先執行上一個cell\n",
    "#open metagenome csv files\n",
    "metagenmoes_df = pd.read_csv('../data/external/genome_metadata_editForAnalysis_NotReference.csv')\n",
    "\n",
    "#merge positive MAGs with metagenome\n",
    "aed_Positive_metagenomes_df = pd.merge(aed_Positive_MAGsID, metagenmoes_df, on='genome_id', how='left')\n",
    "\n",
    "#extract certain column\n",
    "aed_Positive_metagenomes_df = aed_Positive_metagenomes_df[['genome_id', 'metagenome_id', 'taxonomy', 'ecosystem', 'ecosystem_category', 'num_hits', 'longitude', 'latitude']]\n",
    "\n",
    "#extract phylum and class from taxonomy column and expand to new column\n",
    "aed_Positive_metagenomes_df['Phylum'] = aed_Positive_metagenomes_df['taxonomy'].str.extract('(p__\\w+)', expand=True)\n",
    "aed_Positive_metagenomes_df['Class'] = aed_Positive_metagenomes_df['taxonomy'].str.extract('(c__\\w+)', expand=True)\n",
    "\n",
    "#print non-duplicated values in ecosystem_type than check the lable\n",
    "#print(aed_Positive_metagenomes_df['ecosystem_type'].unique())\n",
    "#aed_Positive_metagenomes_df\n",
    "\n",
    "#check the None value in ecosystem column Create a Boolean mask to identify NaN values\n",
    "#mask = aed_addEco_df.isna()\n",
    "#aed_addEco_df_nan_rows = aed_addEco_df[mask.any(axis=1)]\n",
    "#aed_addEco_df_nan_rows\n",
    "\n",
    "#remove, add and rearrange column\n",
    "aed_Positive_metagenomes_df.drop('taxonomy', axis=1, inplace=True)\n",
    "aed_Positive_metagenomes_df = aed_Positive_metagenomes_df.reindex(columns=['genome_id', 'metagenome_id', 'num_hits', 'Phylum', 'Class', 'ecosystem', 'ecosystem_category', 'longitude', 'latitude'])\n",
    "aed_Positive_metagenomes_df['Homologous_cluster']='Actino_aed_cluster'\n",
    "\n",
    "#check phylum data\n",
    "print(aed_Positive_metagenomes_df['Phylum'].unique())\n",
    "print('Any Null in aed_Positive_metagenomes_df:\\n', aed_Positive_metagenomes_df.isnull().any())\n",
    "\n",
    "#write file\n",
    "aed_Positive_metagenomes_df.to_csv('../data/processed/Final/Actino/aed_PositiveHits_ForR_loose.csv')\n",
    "print('Number of p__UBP10: ', aed_Positive_metagenomes_df[aed_Positive_metagenomes_df['Phylum'] == 'p__UBP10'].shape[0])\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b-2 Proteobacteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1b-2a 在 HMMER 分析中用 Bitscore 標準來篩選出相似的 Protein of MAGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proteo_HMM_MAGs\n",
    "# 需執行前一個cell程式\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Aed Cluster to MAGs\n",
    "# Define the directory that contains the \"domtblout\" files.需要刪除discription\n",
    "domtblout_dir = \"../data/raw/Proteo_HMM_MAGs_domtblout/\"\n",
    "\n",
    "# Create an empty dictionary to store the target name for each hmmsearch\n",
    "MAGs_Hits = {}\n",
    "MAGs_Hits_name = []\n",
    "\n",
    "# covert criteria dataframe to serires\n",
    "Proteo_BitScore_Criteria_S = Proteo_BitScore_Criteria['Criteria_Bitscore']\n",
    "Proteo_BitScore_Criteria_S = Proteo_BitScore_Criteria_S.astype(float)  \n",
    "\n",
    "# create a dataframe for a all hits \n",
    "columns = [\"target_name\", \"accession\", \"tlen\", \"query_name\", \"accession2\", \"qlen\", \"E-value\", \"score\", \"bias\",\n",
    "           \"num_domains_index\", \"num_domains_total\", \"c-Evalue\", \"i-Evalue\", \"score2\", \"bias2\", \"hmm_from\",\n",
    "           \"hmm_to\", \"ali_from\", \"ali_to\", \"env_from\", \"env_to\", \"acc\", \"description\"]\n",
    "\n",
    "All_edc_Hits_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Loop over the HMM DOMTBLOUT files and filter the results based on bit score, e-value and coverage\n",
    "# hmm name and bit score are in the Actino_BitScore_Criteria series\n",
    "for hmmsearch, threshold in Proteo_BitScore_Criteria_S.items():\n",
    "    # Load the \"domtblout\" file into a pandas DataFrame\n",
    "    file_path = os.path.join(domtblout_dir, hmmsearch + \".domtblout\")\n",
    "    df = pd.read_csv(file_path, comment=\"#\", sep='\\s+', header=None)    \n",
    "    # Assign column names to the DataFrame\n",
    "    df.columns = [\"target_name\", \"accession\", \"tlen\", \"query_name\", \"accession2\", \"qlen\", \"E-value\", \"score\", \"bias\",\n",
    "                  \"num_domains_index\", \"num_domains_total\", \"c-Evalue\", \"i-Evalue\", \"score2\", \"bias2\", \"hmm_from\", \"hmm_to\", \"ali_from\", \"ali_to\",\n",
    "                  \"env_from\", \"env_to\", \"acc\", \"description\"]\n",
    "\n",
    "    # Calculate the coverage for each hit\n",
    "    df[\"coverage\"] = (df[\"ali_to\"] - df[\"ali_from\"] + 1) / df[\"tlen\"]\n",
    "    \n",
    "    # Filter the DataFrame by E-value, coverage, and bit-score\n",
    "    significant_hits = df[(df[\"E-value\"] < 0.001) & (df[\"coverage\"] > 0.50) & (df[\"score\"] > threshold)]\n",
    "\n",
    "    # Extract Target nmae and store it in the dictionary\n",
    "    if not significant_hits.empty:\n",
    "        MAGs_Hits_name = significant_hits[\"target_name\"].tolist()\n",
    "        MAGs_Hits[hmmsearch] = MAGs_Hits_name\n",
    "    else:\n",
    "        MAGs_Hits_name = None\n",
    "        MAGs_Hits[hmmsearch] = MAGs_Hits_name\n",
    "    \n",
    "    # add hits table to a df\n",
    "    All_edc_Hits_df = pd.concat([significant_hits, All_edc_Hits_df], axis=0)\n",
    "\n",
    "# done\n",
    "print('done')\n",
    "print('unique query name: ', All_edc_Hits_df['query_name'].unique(), ' / ', len(All_edc_Hits_df['query_name'].unique()))\n",
    "All_edc_Hits_df.to_csv('../data/processed/All_edc_Hits_df_bitscore.csv')  \n",
    "All_edc_Hits_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1b-2b 整理出各個 MAGs 所具有的 Protein Hits，再用hits數量進行篩選(>=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Got target name and query name\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "All_edc_Hits_df = pd.read_csv('../data/processed/All_edc_Hits_df_bitscore.csv') \n",
    "All_edc_Hits_TargetAndQuery = All_edc_Hits_df[['target_name', 'query_name']]\n",
    "\n",
    "# Load a Dataframe with the lookup values for merge protein id to MAGs and merge them\n",
    "TarToMAGs_edc = pd.read_csv('../data/interim/edc_All_TarToMAGsID.csv')\n",
    "\n",
    "# use merge() function to join the MAGsID data\n",
    "edc_hits_TargetAndMAGsID = pd.merge(All_edc_Hits_TargetAndQuery, TarToMAGs_edc, on='target_name', how='left')                                                                               \n",
    "\n",
    "# check the null value\n",
    "print('Any Null in TargetToMAGsID: ', edc_hits_TargetAndMAGsID['MAGsID'].isnull().any())\n",
    "\n",
    "# create the crosstab table (like heatmap)\n",
    "edc_hits_heatmap = pd.crosstab(edc_hits_TargetAndMAGsID['query_name'], edc_hits_TargetAndMAGsID['MAGsID'], dropna=False)\n",
    "edc_hits_heatmap = edc_hits_heatmap.transpose()\n",
    "edc_hits_heatmap\n",
    "# edc_hits_heatmap.to_csv('../data/processed/edc_hits_heatmap_bitscore.csv')\n",
    "\n",
    "# Count the non-zero values in hmm profiles hit row to calculate the number of different HMM profiles that have hits in a given MAG. \n",
    "def count_nonzero(row):\n",
    "    return len(row[row != 0])\n",
    "\n",
    "num_hits = edc_hits_heatmap.apply(count_nonzero, axis=1)\n",
    "edc_hits_heatmap['num_hits'] = num_hits\n",
    "\n",
    "# sort them by hits numer\n",
    "edc_hits_heatmap = edc_hits_heatmap.sort_values(by=\"num_hits\", ascending=False)\n",
    "\n",
    "# extract >8 hmm profiles hits and the necessary hits (aedA、aedB、aedJ)\n",
    "edc_hits_FinalFilter =  edc_hits_heatmap[(edc_hits_heatmap['num_hits'] >= 8)]\n",
    "# >7 = 1020, >8 = 597, >9 = 294, >10 = 111 先用8看看 大於一半的query gene\n",
    "\n",
    "# Reset index and move index column to first position\n",
    "edc_hits_FinalFilter.index.name = None\n",
    "edc_hits_FinalFilter = edc_hits_FinalFilter.reset_index()\n",
    "edc_hits_FinalFilter.insert(0, 'index', edc_hits_FinalFilter.pop('index'))\n",
    "\n",
    "# rename MAGsID\n",
    "edc_hits_FinalFilter = edc_hits_FinalFilter.rename(columns={'index': 'genome_id'})\n",
    "\n",
    "# extract the MAGsID and num_hits column\n",
    "edc_Positive_MAGsID = edc_hits_FinalFilter[['genome_id', 'num_hits']]\n",
    "edc_Positive_MAGsID\n",
    "\n",
    "# done\n",
    "print('done')\n",
    "edc_Positive_MAGsID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1b-2c 將篩選出的 MAGS 與相關的資料彙整至 reference 的 MAGs metadata 中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 需先執行上一個cell\n",
    "\n",
    "# open metagenome csv files\n",
    "metagenmoes_df = pd.read_csv('../data/external/genome_metadata_editForAnalysis_NotReference.csv')\n",
    "\n",
    "# merge positive MAGs with metagenome\n",
    "edc_Positive_metagenomes_df = pd.merge(edc_Positive_MAGsID, metagenmoes_df, on='genome_id', how='left')\n",
    "\n",
    "# print(edc_Positive_metagenomes_df['ecosystem_category'].unique())\n",
    "# print(edc_Positive_metagenomes_df['ecosystem_type'].unique())\n",
    "# display(edc_Positive_metagenomes_df[edc_Positive_metagenomes_df['ecosystem_type'] == 'Bacteria']) #Digestive system, Anaerobic\n",
    "\n",
    "# extract certain column\n",
    "edc_Positive_metagenomes_df = edc_Positive_metagenomes_df[['genome_id', 'metagenome_id','taxonomy', 'ecosystem', 'ecosystem_category','num_hits', 'longitude', 'latitude']]\n",
    "\n",
    "# extract phylum and class from taxonomy column and expand to new column\n",
    "edc_Positive_metagenomes_df['Phylum'] = edc_Positive_metagenomes_df['taxonomy'].str.extract('(p__\\w+)', expand=True)\n",
    "edc_Positive_metagenomes_df['Class'] = edc_Positive_metagenomes_df['taxonomy'].str.extract('(c__\\w+)', expand=True)\n",
    "\n",
    "# # print non-duplicated values in ecosystem_type than check the lable\n",
    "# print(edc_Positive_metagenomes_df['ecosystem_type'].unique())\n",
    "\n",
    "# # check the None value in ecosystem column Create a Boolean mask to identify NaN values\n",
    "# mask = edc_addEco_df.isna()\n",
    "# edc_addEco_df_nan_rows = edc_addEco_df[mask.any(axis=1)]\n",
    "# edc_addEco_df_nan_rows\n",
    "\n",
    "# remove, add and rearrange column\n",
    "edc_Positive_metagenomes_df.drop('taxonomy', axis=1, inplace=True)\n",
    "edc_Positive_metagenomes_df = edc_Positive_metagenomes_df.reindex(columns=['genome_id', 'metagenome_id', 'num_hits', 'Phylum', 'Class', 'ecosystem', 'ecosystem_category', 'longitude', 'latitude'])\n",
    "edc_Positive_metagenomes_df['Homologous_cluster']='Proteo_edc_cluster'\n",
    "\n",
    "#check phylum data and null value\n",
    "print(edc_Positive_metagenomes_df['Phylum'].unique())\n",
    "print('Any Null in edc_Positive_metagenomes_df:\\n', edc_Positive_metagenomes_df.isnull().any())\n",
    "\n",
    "# write file\n",
    "edc_Positive_metagenomes_df.to_csv('../data/processed/Final/Proteo/edc_PositiveHits_ForR_loose.csv')\n",
    "print('doen')\n",
    "print('Number of p__UBP10:', edc_Positive_metagenomes_df[edc_Positive_metagenomes_df['Phylum'] == 'p__UBP10'].shape[0])\n",
    "edc_Positive_metagenomes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1c. 處理篩選後的 MAGs 資料，生成要給予 R 分析的資料表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c-1. For Barchart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Read table\n",
    "aed_final_df = pd.read_csv('../data/processed/Final/Actino/aed_PositiveHits_ForR_loose.csv')\n",
    "edc_final_df = pd.read_csv('../data/processed/Final/Proteo/edc_PositiveHits_ForR_loose.csv')\n",
    "\n",
    "# concatenate aed and edc final data\n",
    "B50_HMMFinal_df = pd.concat([aed_final_df, edc_final_df])\n",
    "\n",
    "# remove duplicated and update the 'Both_clsuter' in homologous_cluster column\n",
    "# 找重複 (有2)\n",
    "B50_duplicated = B50_HMMFinal_df['genome_id'].value_counts()\n",
    "\n",
    "# set new column in DataFrame with value counts\n",
    "B50_HMMFinal_df['num_duplicated'] = B50_HMMFinal_df['genome_id'].map(B50_duplicated)\n",
    "\n",
    "# if number is 2 (duplicated) change the homnologous column to 'Both_cluster'\n",
    "B50_HMMFinal_df.loc[B50_HMMFinal_df['num_duplicated'] == 2 , 'Homologous_cluster'] = 'Both_cluster'\n",
    "\n",
    "# remove duplicated row\n",
    "B50_HMMFinal_df = B50_HMMFinal_df.drop_duplicates(subset=['genome_id'])\n",
    "print('B50_final Row (1):', len(B50_HMMFinal_df.index))\n",
    "\n",
    "# remove, add and rearrange column\n",
    "B50_HMMFinal_df.drop('num_duplicated', axis=1, inplace=True)\n",
    "B50_HMMFinal_df = B50_HMMFinal_df.iloc[:, 1:]\n",
    "\n",
    "# open file for vlookup from phylum to final taxonomy ID\n",
    "PhylumToTaxonomy = pd.read_csv('../data/interim/PhylumToTaxonomy.csv')\n",
    "ClassToTaxnomy = pd.read_csv('../data/interim/ClassToTaxonomy.csv')\n",
    "\n",
    "# merge with phylum column\n",
    "B50_HMMFinal_df1 = pd.merge(B50_HMMFinal_df, PhylumToTaxonomy, on='Phylum', how='left')\n",
    "\n",
    "# merge with Class\n",
    "B50_HMMFinal_df2 = pd.merge(B50_HMMFinal_df, ClassToTaxnomy, on='Class', how='left')\n",
    "\n",
    "# concatenate the two dataframes\n",
    "B50_HMMFinal_df = pd.concat([B50_HMMFinal_df1, B50_HMMFinal_df2], axis=0, ignore_index=True)\n",
    "\n",
    "# drop rows with NaN values in taxonomy column\n",
    "B50_HMMFinal_df = B50_HMMFinal_df.dropna(subset=['taxonomy'])\n",
    "print('B50_final Row (2):', len(B50_HMMFinal_df.index))\n",
    "\n",
    "# remove and rerrange column\n",
    "B50_HMMFinal_df.drop(columns=['Phylum', 'Class'], inplace=True)\n",
    "B50_HMMFinal_df = B50_HMMFinal_df.reindex(columns=['genome_id', 'num_hits', 'taxonomy', 'ecosystem', 'ecosystem_category', 'Homologous_cluster', 'longitude', 'latitude'])\n",
    "B50_HMMFinal_df.to_csv('../data/processed/Final/Combined/Combined_PositiveHits_loose.csv')\n",
    "print('Any Null in B50_HMMFinal_df:\\n', B50_HMMFinal_df.isnull().any())\n",
    "\n",
    "# calculate hits number\n",
    "B50_HMMFinal_number = B50_HMMFinal_df.groupby(['ecosystem', 'ecosystem_category', 'taxonomy']).size().reset_index(name='NumberOfMAGs')\n",
    "B50_HMMFinal_number.to_csv('../data/processed/Final/R/B50_HMMFinal_loose_ForBarChart.csv', index=False)\n",
    "\n",
    "print(B50_HMMFinal_number['taxonomy'].unique())\n",
    "print(B50_HMMFinal_number['ecosystem'].unique())\n",
    "print(B50_HMMFinal_number['ecosystem_category'].unique())\n",
    "display(B50_HMMFinal_number)\n",
    "\n",
    "# mmerge reference datasheet to get a full table for readers and Output\n",
    "\n",
    "# read files\n",
    "genome_metadata = pd.read_csv('../data/external/Reference_Data/genome_metadata.csv')\n",
    "genmoe_metadata_edit = pd.read_csv('../data/external/Reference_Data/genome_metadata_edit.csv')\n",
    "genome_metadata_editForAnalysis_NotReference = pd.read_csv('../data/external/genome_metadata_editForAnalysis_NotReference.csv')\n",
    "\n",
    "# Got the specific columns in B50_HMMFinal_df\n",
    "B50_HMMFinal_df_HH = B50_HMMFinal_df[['genome_id', 'num_hits', 'Homologous_cluster']]\n",
    "\n",
    "# merge table\n",
    "M_genome_metadata = pd.merge(B50_HMMFinal_df_HH, genome_metadata, on='genome_id', how='left')\n",
    "M_genmoe_metadata_edit = pd.merge(B50_HMMFinal_df_HH, genmoe_metadata_edit, on='genome_id', how='left')\n",
    "M_genome_metadata_editForAnalysis_NotReference = pd.merge(B50_HMMFinal_df_HH, genome_metadata_editForAnalysis_NotReference, on='genome_id', how='left')\n",
    "\n",
    "# save files\n",
    "M_genome_metadata.to_csv('../data/processed/Final/ForReader/PositiveHits_loose_genome_metadata.csv', index=False)\n",
    "M_genmoe_metadata_edit.to_csv('../data/processed/Final/ForReader/PositiveHits_loose_genmoe_metadata_edit.csv', index=False)\n",
    "M_genome_metadata_editForAnalysis_NotReference.to_csv('../data/processed/Final/ForReader/PositiveHits_loose_genome_metadata_editForAnalysis_NotReference.csv', index=False)\n",
    "\n",
    "# done\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c-2. For Global Distribution Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe kernel 'R' was not started as it is located in an insecure location 'c:\\ProgramData\\jupyter\\kernels\\ir\\kernel.json'.  \n",
      "\u001b[1;31mClick <a href='https://aka.ms/JupyterTrustedKernelPaths'>here</a> for further details, optionally update the setting <a href='command:workbench.action.openSettings?[\"jupyter.kernels.trusted\"]'>jupyter.kernels.trusted</a> to trust the kernel."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Read table\n",
    "aed_final_df = pd.read_csv('../data/processed/Final/Actino/aed_PositiveHits_ForR_loose.csv')\n",
    "edc_final_df = pd.read_csv('../data/processed/Final/Proteo/edc_PositiveHits_ForR_loose.csv')\n",
    "\n",
    "# concatenate aed and edc final data\n",
    "B50_HMMFinal_df = pd.concat([aed_final_df, edc_final_df])\n",
    "\n",
    "# remove duplicated and update the 'Both_clsuter' in homologous_cluster column\n",
    "# 找重複 (有2)\n",
    "B50_duplicated = B50_HMMFinal_df['genome_id'].value_counts()\n",
    "\n",
    "# set new column in DataFrame with value counts\n",
    "B50_HMMFinal_df['num_duplicated'] = B50_HMMFinal_df['genome_id'].map(B50_duplicated)\n",
    "\n",
    "# if number is 2 (duplicated) change the homnologous column to 'Both_cluster'\n",
    "B50_HMMFinal_df.loc[B50_HMMFinal_df['num_duplicated'] == 2 , 'Homologous_cluster'] = 'Both_cluster'\n",
    "\n",
    "# remove duplicated row\n",
    "B50_HMMFinal_df = B50_HMMFinal_df.drop_duplicates(subset=['genome_id'])\n",
    "print('B50_final Row (1):', len(B50_HMMFinal_df.index))\n",
    "\n",
    "# remove, add and rearrange column\n",
    "B50_HMMFinal_df.drop('num_duplicated', axis=1, inplace=True)\n",
    "B50_HMMFinal_df = B50_HMMFinal_df.iloc[:, 1:]\n",
    "\n",
    "# open file for vlookup from phylum to final taxonomy ID\n",
    "PhylumToTaxonomy = pd.read_csv('../data/interim/PhylumToTaxonomy.csv')\n",
    "ClassToTaxnomy = pd.read_csv('../data/interim/ClassToTaxonomy.csv')\n",
    "\n",
    "# merge with phylum column\n",
    "B50_HMMFinal_df1 = pd.merge(B50_HMMFinal_df, PhylumToTaxonomy, on='Phylum', how='left')\n",
    "\n",
    "# merge with Class\n",
    "B50_HMMFinal_df2 = pd.merge(B50_HMMFinal_df, ClassToTaxnomy, on='Class', how='left')\n",
    "\n",
    "# concatenate the two dataframes\n",
    "B50_HMMFinal_df = pd.concat([B50_HMMFinal_df1, B50_HMMFinal_df2], axis=0, ignore_index=True)\n",
    "\n",
    "# drop rows with NaN values in taxonomy column\n",
    "B50_HMMFinal_df = B50_HMMFinal_df.dropna(subset=['taxonomy'])\n",
    "print('B50_final Row (2):', len(B50_HMMFinal_df.index))\n",
    "\n",
    "# remove and rerrange column\n",
    "B50_HMMFinal_df.drop(columns=['Phylum', 'Class', 'num_hits', 'taxonomy', 'Homologous_cluster'], inplace=True)\n",
    "B50_HMMFinal_df = B50_HMMFinal_df.reindex(columns=['genome_id', 'metagenome_id', 'ecosystem', 'ecosystem_category', 'longitude', 'latitude'])\n",
    "\n",
    "# check missing row of coordinates\n",
    "missing_rows = B50_HMMFinal_df[B50_HMMFinal_df[['longitude', 'latitude']].isnull().any(axis=1)]\n",
    "print(f'There are {missing_rows.shape[0]} row(s) with null values.')\n",
    "\n",
    "# Got the metagenomic_id with missing coordinate\n",
    "print('metagenomic_id with missing coordinate: \\n',missing_rows['metagenome_id'].unique )\n",
    "\n",
    "# remove unnecessary columns\n",
    "B50_HMMFinal_df = B50_HMMFinal_df.drop(['genome_id', 'metagenome_id'], axis=1)\n",
    "\n",
    "# rename columns\n",
    "B50_HMMFinal_df = B50_HMMFinal_df.rename(columns={'ecosystem': 'Ecosystem', 'ecosystem_category': 'Ecosystem_Category', 'longitude': 'Longitude', 'latitude': 'Latitude'})\n",
    "\n",
    "# reset index and write files\n",
    "B50_HMMFinal_df = B50_HMMFinal_df.reset_index(drop=True)\n",
    "\n",
    "# assuming your data is in a DataFrame called df\n",
    "B50_HMMFinal_df_grouped = B50_HMMFinal_df.groupby(['Ecosystem', 'Ecosystem_Category', 'Latitude', 'Longitude']).size().reset_index(name='count')\n",
    "B50_HMMFinal_df_grouped.to_csv('../data/processed/Final/R/B50_HMMFinal_loose_ForMaps.csv', index=False)\n",
    "\n",
    "print('Any Null in edc_Positive_metagenomes_df:\\n', B50_HMMFinal_df_grouped.isnull().any())\n",
    "print('done')\n",
    "B50_HMMFinal_df_grouped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. R 圖表製作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a. PercentageBarChart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(ggplot2)\n",
    "library(readr)\n",
    "library(gridExtra)\n",
    "library(patchwork)\n",
    "library(grid)\n",
    "library(cowplot)\n",
    "library(stringr)\n",
    "library(forcats)\n",
    "library(dplyr)\n",
    "\n",
    "# convert the table to a data frame\n",
    "HMM_Final <- read.csv(\"B50_HMMFinal_loose_ForBarChart.csv\", header=TRUE, sep = ',')\n",
    "\n",
    "# Replace all \"Engineered\" values in column \"ecosystem\" with \"Managed\"\n",
    "HMM_Final$ecosystem[HMM_Final$ecosystem == \"Engineered\"] <- \"Managed\"\n",
    "\n",
    "# Replace all \"Non-marine Saline and Alkaline\" values in column \"ecosystem_category\" with \"Soda lake\"\n",
    "HMM_Final$ecosystem_category[HMM_Final$ecosystem_category == \"Non-marine Saline and Alkaline\"] <- \"Soda lake\"\n",
    "\n",
    "# Add new column with percentage values\n",
    "HMM_Final <- HMM_Final %>%\n",
    "  group_by(ecosystem_category) %>%\n",
    "  mutate(percent = NumberOfMAGs / sum(NumberOfMAGs) * 100)\n",
    "HMM_Final\n",
    "\n",
    "\n",
    "# Check unique value of dataframe\n",
    "unique(HMM_Final$ecosystem)\n",
    "unique(HMM_Final$ecosystem_category)\n",
    "unique(HMM_Final$taxonomy)\n",
    "\n",
    "# Change taxonomy order\n",
    "taxonomy_order <- c( \"Unclassified Bacteria\", 'Other Bacterial Phylum','Myxococcia', 'Thermodesulfobacterota',\n",
    "                     \"Chloroflexi\", \"Alphaproteobacteria\", \"Gammaproteobacteria\", \"Actinobacteria\")\n",
    "\n",
    "HMM_Final$taxonomy <- fct_relevel(HMM_Final$taxonomy, taxonomy_order)\n",
    "HMM_Final\n",
    "\n",
    "# change ecosystem order\n",
    "ecosystem_order <- c(\"Aquatic\", \"Terrestrial\", \"Host-associated\", \"Managed\")\n",
    "HMM_Final$ecosystem <- fct_relevel(HMM_Final$ecosystem, ecosystem_order)\n",
    "\n",
    "# create the color vector\n",
    "class_colors <- c(\"Alphaproteobacteria\" = \"#2e75b6\",\n",
    "                  \"Gammaproteobacteria\" = \"#9dc3e6\", \n",
    "                  \"Actinobacteria\" = \"#c00000\",\n",
    "                  \"Chloroflexi\" = \"#a9d18e\",\n",
    "                  'Thermodesulfobacterota' = '#ffd966',\n",
    "                  \"Myxococcia\" = \"#7e33b8\",\n",
    "                  \"Other Bacterial Phylum\" = \"#404040\",\n",
    "                  \"Unclassified Bacteria\" = \"#cbcbcb\"\n",
    ")\n",
    "\n",
    "# ggplot No legend\n",
    "p <- ggplot(HMM_Final, aes(x = str_wrap(ecosystem_category, width = 30), y = percent, fill = taxonomy)) +\n",
    "  geom_bar(stat = \"identity\",\n",
    "           width = ifelse(HMM_Final$ecosystem == \"Terrestrial\", 0.4, 0.8),\n",
    "           position = \"stack\") +\n",
    "  labs(title = \"\",\n",
    "       x =\"\",\n",
    "       y = \"\")+\n",
    "  scale_fill_manual(values = class_colors,\n",
    "                    limits = c(\"Actinobacteria\", \"Chloroflexi\", \"Alphaproteobacteria\", \"Gammaproteobacteria\",\n",
    "                               'Myxococcia', 'Thermodesulfobacterota', 'Other Bacterial Phylum', \"Unclassified Bacteria\"))+\n",
    "  theme_bw()+\n",
    "  theme(plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5),\n",
    "        axis.title.y = element_text(size = 11, margin = margin(r = 10), face = \"bold\"),\n",
    "        strip.text = element_text(size = 11, face = \"bold\"),\n",
    "        axis.text.x = element_text(size = 10, angle = 45, hjust = 1, color =\"black\"),\n",
    "        axis.text.y = element_blank(),\n",
    "        axis.ticks.y = element_blank(),\n",
    "        legend.text = element_text(size = 10),\n",
    "        legend.position = \"none\",\n",
    "        legend.title = element_blank(),\n",
    "        panel.grid.minor.y = element_blank(),\n",
    "        panel.grid.major.x = element_blank(),\n",
    "        plot.margin = unit(c(-0.5, 0.1, -0.5, 0.8), \"cm\"),\n",
    "        text = element_text(family = \"Arial\"))+\n",
    "  scale_y_continuous(expand = c(0.01, 0)) +\n",
    "  facet_wrap(~ecosystem, scales = \"free_x\", nrow = 1)+\n",
    "  guides(fill = guide_legend(ncol = 4))\n",
    "p\n",
    "\n",
    "# save plot with ggsave\n",
    "# unit = 'cm': 高圖參數: (width = 18.18, height = 26.67, dpi = 300) / 矮圖參數: (width = 18.18, height = 7, dpi = 300)\n",
    "ggsave(\"MAGsHits_PercentageBarChart_loose.tiff\", p, width = 18.18, height = 7, dpi = 300, units = \"cm\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b. StackedBarChart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(ggplot2)\n",
    "library(readr)\n",
    "library(gridExtra)\n",
    "library(patchwork)\n",
    "library(grid)\n",
    "library(cowplot)\n",
    "library(stringr)\n",
    "library(forcats)\n",
    "\n",
    "\n",
    "# convert the table to a data frame\n",
    "HMM_Final <- read.csv(\"B50_HMMFinal_loose_ForBarChart.csv\", header=TRUE, sep = ',')\n",
    "\n",
    "# Check unique value of dataframe\n",
    "unique(HMM_Final$ecosystem)\n",
    "unique(HMM_Final$ecosystem_category)\n",
    "unique(HMM_Final$taxonomy)\n",
    "\n",
    "# Replace all \"Engineered\" values in column \"ecosystem\" with \"Managed\"\n",
    "HMM_Final$ecosystem[HMM_Final$ecosystem == \"Engineered\"] <- \"Managed\"\n",
    "\n",
    "# Replace all \"Non-marine Saline and Alkaline\" values in column \"ecosystem_category\" with \"Soda lake\"\n",
    "HMM_Final$ecosystem_category[HMM_Final$ecosystem_category == \"Non-marine Saline and Alkaline\"] <- \"Soda lake\"\n",
    "\n",
    "# Replace \"Alphaproteobacteria\" with \"α-Proteobacteria\"; \"Gammaproteobacteria\" with 'γ-Proteobacteria'; 'Other Bacterial Phylum' with 'Other bacteria'\n",
    "HMM_Final$taxonomy <- gsub(\"Alphaproteobacteria\", \"α-Proteobacteria\", HMM_Final$taxonomy)\n",
    "HMM_Final$taxonomy <- gsub(\"Gammaproteobacteria\", \"γ-Proteobacteria\", HMM_Final$taxonomy)\n",
    "HMM_Final$taxonomy <- gsub(\"Other Bacterial Phylum\", \"Other bacteria\", HMM_Final$taxonomy)\n",
    "HMM_Final$taxonomy <- gsub(\"Unclassified Bacteria\", \"Unclassified bacteria\", HMM_Final$taxonomy)\n",
    "\n",
    "# Change taxonomy order\n",
    "taxonomy_order <- c( \"Unclassified bacteria\", 'Other bacteria','Myxococcia', 'Thermodesulfobacterota',\n",
    "                     \"Chloroflexi\", \"α-Proteobacteria\", \"γ-Proteobacteria\", \"Actinobacteria\")\n",
    "\n",
    "HMM_Final$taxonomy <- fct_relevel(HMM_Final$taxonomy, taxonomy_order)\n",
    "HMM_Final\n",
    "\n",
    "# change ecosystem order\n",
    "ecosystem_order <- c(\"Aquatic\", \"Terrestrial\", \"Host-associated\", \"Managed\")\n",
    "HMM_Final$ecosystem <- fct_relevel(HMM_Final$ecosystem, ecosystem_order)\n",
    "\n",
    "# create the color vector\n",
    "class_colors <- c(\"α-Proteobacteria\" = \"#2e75b6\",\n",
    "                  \"γ-Proteobacteria\" = \"#9dc3e6\", \n",
    "                  \"Actinobacteria\" = \"#c00000\",\n",
    "                  \"Chloroflexi\" = \"#a9d18e\",\n",
    "                  'Thermodesulfobacterota' = '#ffd966',\n",
    "                  \"Myxococcia\" = \"#7e33b8\",\n",
    "                  \"Other bacteria\" = \"#404040\",\n",
    "                  \"Unclassified bacteria\" = \"#cbcbcb\"\n",
    ")\n",
    "\n",
    "# ggplot No legend\n",
    "p <- ggplot(HMM_Final, aes(x = str_wrap(ecosystem_category, width = 30), y = NumberOfMAGs, fill = taxonomy)) +\n",
    "  geom_bar(stat = \"identity\",\n",
    "           width = ifelse(HMM_Final$ecosystem == \"Terrestrial\", 0.4, 0.8),\n",
    "           position = \"stack\") +\n",
    "  labs(title = \"\",\n",
    "       x =\"\",\n",
    "       y = \"\")+\n",
    "  scale_fill_manual(values = class_colors,\n",
    "                    limits = c(\"Actinobacteria\", \"Chloroflexi\", \"α-Proteobacteria\", \"γ-Proteobacteria\",\n",
    "                               'Myxococcia', 'Thermodesulfobacterota', 'Other bacteria', \"Unclassified bacteria\"))+\n",
    "  theme_bw()+\n",
    "  theme(plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5),\n",
    "        axis.title.y = element_text(size = 11, margin = margin(r = 10), face = \"bold\"),\n",
    "        strip.text = element_text(size = 11, face = \"bold\"),\n",
    "        axis.text.x = element_text(size = 10, angle = 45, hjust = 1, color =\"black\"),\n",
    "        axis.text.y = element_text(size = 10),\n",
    "        legend.text = element_text(size = 10),\n",
    "        legend.position = \"none\",\n",
    "        legend.title = element_blank(),\n",
    "        panel.grid.minor.y = element_blank(),\n",
    "        panel.grid.major.x = element_blank(),\n",
    "        plot.margin = unit(c(-0.5, 0.1, -0.5, 0.1), \"cm\"),\n",
    "        text = element_text(family = \"Arial\"))+\n",
    "  scale_y_continuous(expand = c(0.03, 0), breaks = seq(0, 300, 50)) +\n",
    "  facet_wrap(~ecosystem, scales = \"free_x\", nrow = 1)+\n",
    "  guides(fill = guide_legend(ncol = 4))\n",
    "p\n",
    "\n",
    "# save plot with ggsave\n",
    "# unit = 'cm': 高圖參數: (width = 18.18, height = 26.67, dpi = 300) / 矮圖參數: (width = 18.18, height = 7, dpi = 300)\n",
    "ggsave(\"MAGsHits_StackedBarChart_loose.tiff\", p, width = 18.18, height = 7, dpi = 300, units = \"cm\")\n",
    "\n",
    "\n",
    "#-----------------------\n",
    "# ggplot with legend\n",
    "pl <- ggplot(HMM_Final, aes(x = str_wrap(ecosystem_category, width = 30), y = NumberOfMAGs, fill = taxonomy)) +\n",
    "  geom_bar(stat = \"identity\",\n",
    "           width = ifelse(HMM_Final$ecosystem == \"Terrestrial\", 0.4, 0.8),\n",
    "           position = \"stack\") +\n",
    "  labs(title = \"\",\n",
    "       x =\"\",\n",
    "       y = \"Number of estrogen-degrading MAGs\")+\n",
    "  scale_fill_manual(values = class_colors,\n",
    "                    limits = c(\"Actinobacteria\", \"Chloroflexi\", \"α-Proteobacteria\", \"γ-Proteobacteria\",\n",
    "                               'Myxococcia', 'Thermodesulfobacterota', 'Other bacteria', \"Unclassified bacteria\"))+\n",
    "  theme_bw()+\n",
    "  theme(plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5),\n",
    "        axis.title.y = element_text(size = 12, margin = margin(r = 10), face = \"bold\"),\n",
    "        strip.text = element_text(size = 12, face = \"bold\"),\n",
    "        axis.text.x = element_text(size = 10, angle = 45, hjust = 1, color =\"black\"),\n",
    "        axis.text.y = element_text(size = 10),\n",
    "        legend.text = element_text(size = 10),\n",
    "        legend.position = \"bottom\",\n",
    "        legend.title = element_blank(),\n",
    "        panel.grid.minor.y = element_blank(),\n",
    "        panel.grid.major.x = element_blank(),\n",
    "        plot.margin = unit(c(0.5, 0.1, 0.5, 0.1), \"cm\"),\n",
    "        text = element_text(family = \"Arial\"))+\n",
    "  scale_y_continuous(expand = c(0.03, 0), breaks = seq(0, 300, 50)) +\n",
    "  facet_wrap(~ecosystem, scales = \"free_x\", nrow = 1)+\n",
    "  guides(fill = guide_legend(ncol = 4))\n",
    "pl\n",
    "# save plot for legend\n",
    "ggsave(\"MAGsHits_BarChart_loose_legend.tiff\", pl, width = 25.4, height = 26.67, dpi = 300, units = \"cm\")\n",
    "\n",
    "# done\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2c. Globla Distibution Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load required packages\n",
    "library(ggplot2)\n",
    "library(sf)\n",
    "library(maps)\n",
    "library(dplyr)\n",
    "library(forcats)\n",
    "library(cowplot)\n",
    "library(scales)\n",
    "\n",
    "# read table\n",
    "df <- read.csv(\"B50_HMMFinal_loose_ForMaps.csv\")\n",
    "df\n",
    "\n",
    "# Replace all \"Non-marine Saline and Alkaline\" values in column \"ecosystem_category\" with \"Salt Lake\"\n",
    "df$Ecosystem_Category[df$Ecosystem_Category == \"Non-marine Saline and Alkaline\"] <- \"Soda lake\"\n",
    "\n",
    "# Get unique ecosystem names\n",
    "ecosystems <- unique(df$Ecosystem)\n",
    "\n",
    "# Create empty list to store dataframes\n",
    "df_list <- list()\n",
    "\n",
    "# Loop through each ecosystem and create a new dataframe for each\n",
    "for (i in ecosystems) {\n",
    "  temp_df <- df[df$Ecosystem == i, ]\n",
    "  df_list[[i]] <- temp_df\n",
    "}\n",
    "\n",
    "# View list of dataframes\n",
    "df_list\n",
    "\n",
    "# rename seperate list\n",
    "Aquatic_df <- do.call(rbind, df_list[1])\n",
    "Engineered_df <- do.call(rbind, df_list[2])\n",
    "Host_associate_df <- do.call(rbind, df_list[3])\n",
    "Terrestrial_df <- do.call(rbind, df_list[4])\n",
    "\n",
    "# Convert the dataframe to an sf object and set coordinates\n",
    "Aquatic_sf <- st_as_sf(Aquatic_df, coords = c(\"Longitude\", \"Latitude\"), crs = 4326)\n",
    "Engineered_sf <- st_as_sf(Engineered_df, coords = c(\"Longitude\", \"Latitude\"), crs = 4326)\n",
    "Host_associate_sf <- st_as_sf(Host_associate_df, coords = c(\"Longitude\", \"Latitude\"), crs = 4326)\n",
    "Terrestrial_sf <- st_as_sf(Terrestrial_df, coords = c(\"Longitude\", \"Latitude\"), crs = 4326)\n",
    "\n",
    "#---------------------------\n",
    "# plot Aquatic map\n",
    "\n",
    "# change ecosystem_category order\n",
    "ecosystem_category_order <- c(\"Marine\", \"Freshwater\", \"Salt Lake\", \"Sediment\", 'Thermal springs')\n",
    "Aquatic_sf$Ecosystem_Category <- fct_relevel(Aquatic_sf$Ecosystem_Category, ecosystem_category_order)\n",
    "\n",
    "# create the color vector\n",
    "Aquatic_colors <- c(\"Marine\" = \"#1e4c9c\",\n",
    "                    \"Freshwater\" = \"#4286bf\",\n",
    "                    \"Salt Lake\" = \"#83b4d3\", \n",
    "                    \"Sediment\" = \"#404040\",\n",
    "                    'Thermal springs'= '#7f7f7f'\n",
    ")\n",
    "\n",
    "# Plot\n",
    "Aquatic_map <-\n",
    "  ggplot(data=world)+\n",
    "  geom_sf(fill='#dadada', color='#dadada')+\n",
    "  geom_sf(data=Aquatic_sf, aes(color = Ecosystem_Category,\n",
    "                               size = count))+\n",
    "  theme_void()+\n",
    "  theme(\n",
    "    legend.text = element_text(size = 12),\n",
    "    legend.title = element_blank(),\n",
    "    legend.position = 'bottom',\n",
    "    plot.margin = unit(c(0, 0.3, 0, 0.1), \"cm\"),\n",
    "    legend.margin = margin(t = -0.5, r = 0, b = 0, l = 0, unit = \"cm\")\n",
    "  )+\n",
    "  scale_size_continuous(breaks = c(1, 8, 15), labels = c(\"1\", \"8\", \"15\"), name = \"Count\")+\n",
    "  scale_color_manual(values = Aquatic_colors,\n",
    "                     label = c('Marine', 'Freshwater', 'Salt Lake', 'Sediment', 'Thermal springs'),\n",
    "                     guide = guide_legend(nrow = 2))\n",
    "\n",
    "Aquatic_map\n",
    "\n",
    "# save plot\n",
    "ggsave(\"loose_Aquatic_Maps_Main.tiff\", Aquatic_map, width = 18.18, height = 8, dpi = 300, units = \"cm\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
