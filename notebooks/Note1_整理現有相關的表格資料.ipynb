{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20810aec-fb42-44ea-8265-7bf385bad544",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Note1_整理與修正 reference 表格\n",
    "現有的metagenome metatable 本身少了 ecosystem 的欄位資料，需要從該paper的supplemetary table 中的 metagenome table 來進行資料的修正，接著還需要做一張表格是給本次分析，要把terrestrial與aquatic等標籤換至ecosystem中，而不是繼續在ecosystem_category中。\n",
    "\n",
    "成果:\n",
    "Reference table 與本次修改的table之間的唯一差別是在 ecosystem的部分，reference中terrestrial 與 aquatic 有著 ecosystem階層: environment，但修改後的版本是把environment給移除，terrestrial與aquatic皆上一一階層。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7163b621-f0e5-44f7-80d8-93e4f7ada3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# link ecosystem information of IMG_metafata to genome_metadata\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# read files\n",
    "genome_metadata = pd.read_csv('../data/external/Reference_Data/genome_metadata.csv')\n",
    "IMG_metadata = pd.read_csv('../data/external/Reference_Data/IMG_metadata.csv')\n",
    "\n",
    "# extract IMG_TAXON_ID and ECOSYSTEM columns of IMG_metadata\n",
    "IMG_metadata_IDECO = IMG_metadata[['IMG_TAXON_ID', 'ECOSYSTEM']]\n",
    "\n",
    "# rename columns\n",
    "IMG_metadata_IDECO = IMG_metadata_IDECO.rename(columns=({'IMG_TAXON_ID':'metagenome_id'}))\n",
    "\n",
    "# merge two dataframe on metagenome_id\n",
    "genome_metadata_ECO = pd.merge(genome_metadata, IMG_metadata_IDECO, on='metagenome_id', how='left')\n",
    "\n",
    "# rearrange columns\n",
    "genome_metadata_ECO = genome_metadata_ECO[['genome_id', 'metagenome_id', 'genome_length', 'num_contigs', 'n50',\n",
    "       'num_16s', 'num_5s', 'num_23s', 'num_trna', 'completeness',\n",
    "       'contamination', 'quality_score', 'mimag_quality', 'otu_id',\n",
    "       'ecosystem', 'ECOSYSTEM', 'ecosystem_category', 'ecosystem_type', 'habitat',\n",
    "       'longitude', 'latitude']]\n",
    "\n",
    "# rename\n",
    "genome_metadata_ECO = genome_metadata_ECO.rename(columns=({'ecosystem':'taxonomy', 'ECOSYSTEM':'ecosystem'}))\n",
    "\n",
    "# save files\n",
    "genome_metadata_ECO.to_csv('../data/external/Reference_Data/genome_metadata_edit.csv', index=False)\n",
    "\n",
    "# Next step will perform with excel to change the order about terrestrial and aquatic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93e65ed-f535-4c05-880c-a1ebfadffa3c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 舊資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f346b578-ec12-43cd-9ae2-a9d426525bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7304\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "Genome_metadata = pd.read_csv('../data/external/Reference_Data/genome_metadata.csv')\n",
    "print(len(Genome_metadata['metagenome_id'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7413feb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "date: 2023/02/15\n",
    "\n",
    "---\n",
    "\n",
    "## Note1_目標\n",
    "\n",
    "整理現有相關的資料，包含metagenomes相關的表格資料與HMM分析相關的raw data:\n",
    "\n",
    "> metagenomes 表格資料\n",
    ">> - Paper相關資料  \n",
    "    - Paper: [A genomic catalog of Earth’s microbiomes](https://www.nature.com/articles/s41587-020-0718-6)  \n",
    "    - Download site: [https://portal.nersc.gov/GEM/](https://portal.nersc.gov/GEM/)  \n",
    "    - tsv files: ./data/external/Paper_genome_metadata.tsv\n",
    ">> - 已添加 Biomes data 到 metagenomes 的Table:\n",
    "    - 目前失敗，有個暫存檔 (BiomesToSub_ecosystem.csv) 在 data/intermin 中\n",
    "    \n",
    "> HMM 標準設置資料 (Positive與Negative control)\n",
    ">> - 執行紀錄: B50_HMM_SettingCriteria_20230203 (Folder)\n",
    ">> - 相關 HMM output data\n",
    "    - Actinobacteria  \n",
    "    Positive control HMM domtblout: 0  \n",
    "    Negative control HMM domtblout: 0\n",
    "    \n",
    "## HMM 標準設置\n",
    "參考 p-fam 的設置，e-value 為0.001，Database中的序列seaech 設為45638612; -E 0.001 (e-7) -Z 45638612。coverage 設置要大於 50%，可以從domtblout 的結果中用 (ali_to - ali_from + 1) / dtlen) 來計算。最後以negative control來針對各個 hmm profiels設置相應的 bit-score\n",
    "\n",
    "### Negative 測試\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8392183b-ce2a-4c4e-8109-b4c8b708c22b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MAGs_A_aedC_RS26365      450.3\n",
       "MAGs_A_aedD_RS26370      128.9\n",
       "MAGs_A_aedE_RS26375      232.1\n",
       "MAGs_A_aedI_RS26405       25.4\n",
       "MAGs_A_aedL_RS26420      106.7\n",
       "MAGs_A_aedM_RS26425       98.3\n",
       "MAGs_A_aedN_RS26430      221.1\n",
       "MAGs_A_aedO_RS26435       58.9\n",
       "MAGs_A_aedP_RS26440      181.3\n",
       "MAGs_A_aedQ_RS26445       42.4\n",
       "MAGs_A_aedR_RS26450      255.5\n",
       "MAGs_A_I_aedA_RS26385    195.4\n",
       "MAGs_A_I_aedB_RS26395    220.3\n",
       "MAGs_A_I_aedF_RS26380    617.3\n",
       "MAGs_A_I_aedG_RS26390    176.2\n",
       "MAGs_A_I_aedH_RS26400    270.8\n",
       "MAGs_A_I_aedJ_RS26410    328.3\n",
       "MAGs_A_I_aedK_RS26415    321.5\n",
       "dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HMM actino negative control bit-score threshold\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# A_Rhodococcus_jostii_RHA1\n",
    "# Define the directory that contains the \"domtblout\" files, 這個genome protein fasta要把discription的文字刪除，因有空格會影響read\n",
    "domtblout_dir = \"../data/raw/Actino_HMM_Control/Negative/A_Rhodococcus_jostii_RHA1/domtblout/\"\n",
    "\n",
    "# Define the names of the hmmsearches (without the file extension)\n",
    "hmmsearches = [\"A_Rhodococcus_jostii_RHA1_aedC_RS26365\", \"A_Rhodococcus_jostii_RHA1_aedD_RS26370\", \"A_Rhodococcus_jostii_RHA1_aedE_RS26375\", \"A_Rhodococcus_jostii_RHA1_aedI_RS26405\", \"A_Rhodococcus_jostii_RHA1_aedL_RS26420\", \"A_Rhodococcus_jostii_RHA1_aedM_RS26425\", \"A_Rhodococcus_jostii_RHA1_aedN_RS26430\", \"A_Rhodococcus_jostii_RHA1_aedO_RS26435\", \"A_Rhodococcus_jostii_RHA1_aedP_RS26440\", \"A_Rhodococcus_jostii_RHA1_aedQ_RS26445\", \"A_Rhodococcus_jostii_RHA1_aedR_RS26450\", \"A_Rhodococcus_jostii_RHA1_I_aedA_RS26385\", \"A_Rhodococcus_jostii_RHA1_I_aedB_RS26395\", \"A_Rhodococcus_jostii_RHA1_I_aedF_RS26380\", \"A_Rhodococcus_jostii_RHA1_I_aedG_RS26390\", \"A_Rhodococcus_jostii_RHA1_I_aedH_RS26400\", \"A_Rhodococcus_jostii_RHA1_I_aedJ_RS26410\", \"A_Rhodococcus_jostii_RHA1_I_aedK_RS26415\"]\n",
    "\n",
    "# Create an empty dictionary to store the best bit-scores for each hmmsearch\n",
    "best_bit_scores = {}\n",
    "\n",
    "# Loop over the hmmsearches and parse the corresponding \"domtblout\" file\n",
    "for hmmsearch in hmmsearches:\n",
    "    # Load the \"domtblout\" file into a pandas DataFrame\n",
    "    file_path = os.path.join(domtblout_dir, hmmsearch + \".domtblout\")\n",
    "    df = pd.read_csv(file_path, comment=\"#\", sep='\\s+', header=None)    \n",
    "    # Assign column names to the DataFrame\n",
    "    df.columns = [\"target_name\", \"accession\", \"tlen\", \"query_name\", \"accession2\", \"qlen\", \"E-value\", \"score\", \"bias\",\n",
    "                  \"num_domains_index\", \"num_domains_total\", \"c-Evalue\", \"i-Evalue\", \"score2\", \"bias2\", \"hmm_from\", \"hmm_to\", \"ali_from\", \"ali_to\",\n",
    "                  \"env_from\", \"env_to\", \"acc\", \"description\"]\n",
    "    # Calculate the coverage for each hit\n",
    "    df[\"coverage\"] = (df[\"ali_to\"] - df[\"ali_from\"] + 1) / df[\"tlen\"]\n",
    "\n",
    "    # Filter the DataFrame by E-value and coverage, and sort by bit-score\n",
    "    significant_hits = df[(df[\"E-value\"] <= 0.001) & (df[\"coverage\"] > 0.50)].sort_values(by=\"score\", ascending=False)\n",
    "    \n",
    "    # replace strains name to aed..\n",
    "    pattern = r'A_Rhodococcus_jostii_RHA1_'\n",
    "    hmmsearch = re.sub(pattern, 'A_', hmmsearch)\n",
    "    \n",
    "    # Extract the best bit-score and store it in the dictionary\n",
    "    best_bit_score = significant_hits.iloc[0][\"score\"]\n",
    "    best_bit_scores[hmmsearch] = best_bit_score\n",
    "\n",
    "# Convert the dictionary to a pandas DataFrame and transpose it to get the summary table\n",
    "summary_table_R = pd.DataFrame(best_bit_scores, index=[\"best_bit_score_R\"])\n",
    "summary_table_R.insert(0, 'Strains', ['A_Rhodococcus_jostii_RHA1_A'])\n",
    "summary_table_R\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# A_Mycobacterium_tuberculosis_H37Rv\n",
    "# Define the directory that contains the \"domtblout\" files, 這個genome protein fasta要把discription的文字刪除，因有空格會影響read\n",
    "domtblout_dir = \"../data/raw/Actino_HMM_Control/Negative/A_Mycobacterium_tuberculosis_H37Rv/domtblout/\"\n",
    "\n",
    "# Define the names of the hmmsearches (without the file extension)\n",
    "hmmsearches = ['A_Mycobacterium_tuberculosis_H37Rv_aedC_RS26365', 'A_Mycobacterium_tuberculosis_H37Rv_aedD_RS26370', 'A_Mycobacterium_tuberculosis_H37Rv_aedE_RS26375', 'A_Mycobacterium_tuberculosis_H37Rv_aedI_RS26405', 'A_Mycobacterium_tuberculosis_H37Rv_aedL_RS26420', 'A_Mycobacterium_tuberculosis_H37Rv_aedM_RS26425', 'A_Mycobacterium_tuberculosis_H37Rv_aedN_RS26430', 'A_Mycobacterium_tuberculosis_H37Rv_aedO_RS26435', 'A_Mycobacterium_tuberculosis_H37Rv_aedP_RS26440', 'A_Mycobacterium_tuberculosis_H37Rv_aedQ_RS26445', 'A_Mycobacterium_tuberculosis_H37Rv_aedR_RS26450', 'A_Mycobacterium_tuberculosis_H37Rv_I_aedA_RS26385', 'A_Mycobacterium_tuberculosis_H37Rv_I_aedB_RS26395', 'A_Mycobacterium_tuberculosis_H37Rv_I_aedF_RS26380', 'A_Mycobacterium_tuberculosis_H37Rv_I_aedG_RS26390', 'A_Mycobacterium_tuberculosis_H37Rv_I_aedH_RS26400', 'A_Mycobacterium_tuberculosis_H37Rv_I_aedJ_RS26410', 'A_Mycobacterium_tuberculosis_H37Rv_I_aedK_RS26415']\n",
    "# Create an empty dictionary to store the best bit-scores for each hmmsearch\n",
    "best_bit_scores = {}\n",
    "\n",
    "# Loop over the hmmsearches and parse the corresponding \"domtblout\" file\n",
    "for hmmsearch in hmmsearches:\n",
    "    # Load the \"domtblout\" file into a pandas DataFrame\n",
    "    file_path = os.path.join(domtblout_dir, hmmsearch + \".domtblout\")\n",
    "    df = pd.read_csv(file_path, comment=\"#\", sep='\\s+', header=None)    \n",
    "    # Assign column names to the DataFrame\n",
    "    df.columns = [\"target_name\", \"accession\", \"tlen\", \"query_name\", \"accession2\", \"qlen\", \"E-value\", \"score\", \"bias\",\n",
    "                  \"num_domains_index\", \"num_domains_total\", \"c-Evalue\", \"i-Evalue\", \"score2\", \"bias2\", \"hmm_from\", \"hmm_to\", \"ali_from\", \"ali_to\",\n",
    "                  \"env_from\", \"env_to\", \"acc\", \"description\"]\n",
    "    # Calculate the coverage for each hit\n",
    "    df[\"coverage\"] = (df[\"ali_to\"] - df[\"ali_from\"] + 1) / df[\"tlen\"]\n",
    "\n",
    "    # Filter the DataFrame by E-value and coverage, and sort by bit-score\n",
    "    significant_hits_1 = df[(df[\"E-value\"] <= 0.001) & (df[\"coverage\"] > 0.50)].sort_values(by=\"score\", ascending=False)\n",
    "   \n",
    "    # replace strains name to aed..\n",
    "    pattern = r'A_Mycobacterium_tuberculosis_H37Rv_'\n",
    "    hmmsearch = re.sub(pattern, 'A_', hmmsearch)\n",
    "\n",
    "    # Extract the best bit-score and store it in the dictionary\n",
    "    if not significant_hits_1.empty:\n",
    "        best_bit_score = significant_hits_1.iloc[0][\"score\"]\n",
    "        best_bit_scores[hmmsearch] = best_bit_score\n",
    "    else:\n",
    "        best_bit_score = 0\n",
    "        best_bit_scores[hmmsearch] = best_bit_score\n",
    "\n",
    "# Convert the dictionary to a pandas DataFrame and transpose it to get the summary table\n",
    "summary_table_M = pd.DataFrame(best_bit_scores, index=[\"best_bit_score_M\"])\n",
    "summary_table_M.insert(0, 'Strains', ['Mycobacterium_tuberculosis_H37Rv'])\n",
    "summary_table_M\n",
    "\n",
    "# merge two negative best bit score df\n",
    "Actino_Negative_df = pd.concat([summary_table_R, summary_table_M], axis=0)\n",
    "Actino_Negative_df\n",
    "\n",
    "# Got the smallest bit score of the df\n",
    "Actino_BitScore_Criteria = Actino_Negative_df.max()\n",
    "Actino_BitScore_Criteria = Actino_BitScore_Criteria.iloc[1:]\n",
    "\n",
    "# Change index name for MAGs data parse\n",
    "for old_index in Actino_BitScore_Criteria.index:\n",
    "    Actino_BitScore_Criteria.rename(index={old_index: re.sub('A_aed', 'MAGs_A_aed', old_index)}, inplace=True)\n",
    "\n",
    "for old_index in Actino_BitScore_Criteria.index:\n",
    "    Actino_BitScore_Criteria.rename(index={old_index: re.sub('A_I_aed', 'MAGs_A_I_aed', old_index)}, inplace=True)\n",
    "    \n",
    "Actino_BitScore_Criteria = Actino_BitScore_Criteria.astype(float)\n",
    "    \n",
    "Actino_BitScore_Criteria\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "af21bcfe-1285-451e-8e6e-a78bd70f833d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actino_HMM_MAGs\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Aed Cluster to MAGs\n",
    "# Define the directory that contains the \"domtblout\" files.需要刪除discription\n",
    "domtblout_dir = \"../data/raw/Actino_HMM_MAGs_domtblout/\"\n",
    "\n",
    "# Create an empty dictionary to store the target name for each hmmsearch\n",
    "MAGs_Hits = {}\n",
    "MAGs_Hits_name = []\n",
    "\n",
    "# delet aedC,Q,R. They are not in next hmm analysis\n",
    "to_drop = ['MAGs_A_aedC_RS26365', 'MAGs_A_aedQ_RS26445', 'MAGs_A_aedR_RS26450']\n",
    "Actino_BitScore_Criteria = Actino_BitScore_Criteria.loc[~Actino_BitScore_Criteria.index.isin(to_drop)]\n",
    "\n",
    "# create a dataframe for a all hits \n",
    "columns = [\"target_name\", \"accession\", \"tlen\", \"query_name\", \"accession2\", \"qlen\", \"E-value\", \"score\", \"bias\",\n",
    "           \"num_domains_index\", \"num_domains_total\", \"c-Evalue\", \"i-Evalue\", \"score2\", \"bias2\", \"hmm_from\",\n",
    "           \"hmm_to\", \"ali_from\", \"ali_to\", \"env_from\", \"env_to\", \"acc\", \"description\"]\n",
    "\n",
    "All_aed_Hits_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Loop over the HMM DOMTBLOUT files and filter the results based on bit score, e-value and coverage\n",
    "# hmm name and bit score are in the Actino_BitScore_Criteria series\n",
    "for hmmsearch, threshold in Actino_BitScore_Criteria.items():\n",
    "    # Load the \"domtblout\" file into a pandas DataFrame\n",
    "    file_path = os.path.join(domtblout_dir, hmmsearch + \".domtblout\")\n",
    "    df = pd.read_csv(file_path, comment=\"#\", sep='\\s+', header=None)    \n",
    "    # Assign column names to the DataFrame\n",
    "    df.columns = [\"target_name\", \"accession\", \"tlen\", \"query_name\", \"accession2\", \"qlen\", \"E-value\", \"score\", \"bias\",\n",
    "                  \"num_domains_index\", \"num_domains_total\", \"c-Evalue\", \"i-Evalue\", \"score2\", \"bias2\", \"hmm_from\", \"hmm_to\", \"ali_from\", \"ali_to\",\n",
    "                  \"env_from\", \"env_to\", \"acc\", \"description\"]\n",
    "\n",
    "    # Calculate the coverage for each hit\n",
    "    df[\"coverage\"] = (df[\"ali_to\"] - df[\"ali_from\"] + 1) / df[\"tlen\"]\n",
    "    \n",
    "    # Filter the DataFrame by E-value, coverage, and bit-score\n",
    "    significant_hits = df[(df[\"E-value\"] <= 0.001) & (df[\"coverage\"] > 0.50) & (df[\"score\"] > threshold)]\n",
    "\n",
    "    # Extract Target nmae and store it in the dictionary\n",
    "    if not significant_hits.empty:\n",
    "        MAGs_Hits_name = significant_hits[\"target_name\"].tolist()\n",
    "        MAGs_Hits[hmmsearch] = MAGs_Hits_name\n",
    "    else:\n",
    "        MAGs_Hits_name = None\n",
    "        MAGs_Hits[hmmsearch] = MAGs_Hits_name\n",
    "    \n",
    "    # add hits table to a df\n",
    "    All_aed_Hits_df = pd.concat([significant_hits, All_aed_Hits_df], axis=0)\n",
    "\n",
    "# All_aed_Hits_df.to_csv('../data/processed/All_aed_Hits_df.csv')    \n",
    "    \n",
    "All_aed_Hits_TargetAndQuery = All_aed_Hits_df[['target_name', 'query_name']]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c117dd-a5b1-4424-bf0f-5fd740da2b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use python to get the MAGs ID of Target gene\n",
    "\n",
    "# import os\n",
    "# import re\n",
    "# import time\n",
    "# import glob\n",
    "# # aed_All target gene\n",
    "\n",
    "# time_start = time.time()\n",
    "# count = 0\n",
    "# aed_All_HitGenome = open(\"..\\data\\interim\\actino_HMMHitGeneToMAGsID.txt\", 'w+', encoding = 'utf-8') #建立檔案\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2cbcba3a-ce9a-4ec5-9eac-83d1bfc05c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# link target name to metagenome dataframe\n",
    "\n",
    "# Load a Dataframe with the lookup values for merge protein id to MAGs and merge them\n",
    "TarToMAGs_aed = pd.read_csv('../data/interim/aed_TargetToMAGsID_all.csv')\n",
    "\n",
    "# use merge() function to join the MAGsID data\n",
    "aed_hits_TargetAndMAGsID = pd.merge(All_aed_Hits_TargetAndQuery, TarToMAGs_aed, on='target_name', how='left')                                                                               \n",
    "aed_hits_TargetAndMAGsID\n",
    "\n",
    "# # remove duplicates in the 'MAGsID' column\n",
    "# a = aed_hits_TargetAndMAGsID['query_name'].drop_duplicates()\n",
    "# a\n",
    "\n",
    "# # Find the rows with at least one NaN value\n",
    "# nan_rows = aed_hits_TargetAndMAGsID.isna().any(axis=1)\n",
    "# aed_hits_nan_rows = aed_hits_TargetAndMAGsID[nan_rows]\n",
    "# aed_hits_nan_rows.to_csv('../data/interim/aed_hits_nan_rows.csv')\n",
    "\n",
    "# create the crosstab table (like heatmap)\n",
    "aed_hits_heatmap = pd.crosstab(aed_hits_TargetAndMAGsID['query_name'], aed_hits_TargetAndMAGsID['MAGsID'], dropna=False)\n",
    "aed_hits_heatmap = aed_hits_heatmap.transpose()\n",
    "aed_hits_heatmap.to_csv('../data/processed/aed_hits_heatmap_bitscore.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d20591-3fb5-42ad-aed6-7e936a59cde5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 下列為暫不執行的cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acaaf3a3-4267-48d8-977b-c23973697dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "TarToMAGs_null = pd.read_csv('../data/interim/null_TarToMAGs.csv')\n",
    "TarToMAGs_aed_All = pd.read_csv('../data/interim/Actino_aed/aed_All_TarToMAGsID.csv')\n",
    "TarToMAGs_aed_All = pd.concat([TarToMAGs_aed_All, TarToMAGs_null], axis=0)\n",
    "TarToMAGs_aed_All.to_csv('../data/interim/Actino_aed/aed_All_TarToMAGsID.csv')\n",
    "print('done')\n",
    "# TarToMAGs_aedCB = pd.read_csv('../data/interim/aedC_aedB.csv')\n",
    "# TarToMAGs_aedHR = pd.read_csv('../data/interim/aedH_aedR.csv')\n",
    "# TarToMAGs_aed_Non = pd.read_csv('../data/interim/aed_Non_TarToMAGsID.csv')\n",
    "# TarToMAGs_aed_All = pd.concat([TarToMAGs_aedCB, TarToMAGs_aedHR], axis=0)\n",
    "# TarToMAGs_aed_All = pd.concat([TarToMAGs_aed_All, TarToMAGs_aed_Non], axis=0)\n",
    "# print(TarToMAGs_aed_All.shape[0])\n",
    "# TarToMAGs_aed_All\n",
    "# TarToMAGs_aed_All = TarToMAGs_aed_All.drop_duplicates(subset='target_name')\n",
    "# print(TarToMAGs_aed_All.shape[0])\n",
    "# TarToMAGs_aed_All.to_csv('../data/interim/aed_All_TarToMAGsID.csv')\n",
    "# # TarToMAGs_aed_All.to_csv('../data/interim/aed_TargetToMAGsID_all.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # All_aed_Hits_TargetAndQuery\n",
    "\n",
    "\n",
    "# # for test\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# import re\n",
    "\n",
    "# # Aed Cluster to MAGs\n",
    "# # Define the directory that contains the \"domtblout\" files.需要刪除discription\n",
    "# domtblout_dir = \"../data/raw/Actino_HMM_MAGs_domtblout/\"\n",
    "\n",
    "# # Create an empty dictionary to store the target name for each hmmsearch\n",
    "# MAGs_Hits = {}\n",
    "# MAGs_Hits_name = []\n",
    "\n",
    "# # delet aedC,Q,R. They are not in next hmm analysis\n",
    "# to_drop = ['MAGs_A_aedC_RS26365', 'MAGs_A_aedQ_RS26445', 'MAGs_A_aedR_RS26450']\n",
    "# Actino_BitScore_Criteria = Actino_BitScore_Criteria.loc[~Actino_BitScore_Criteria.index.isin(to_drop)]\n",
    "\n",
    "# # create a dataframe for a all hits \n",
    "# columns = [\"target_name\", \"accession\", \"tlen\", \"query_name\", \"accession2\", \"qlen\", \"E-value\", \"score\", \"bias\",\n",
    "#            \"num_domains_index\", \"num_domains_total\", \"c-Evalue\", \"i-Evalue\", \"score2\", \"bias2\", \"hmm_from\",\n",
    "#            \"hmm_to\", \"ali_from\", \"ali_to\", \"env_from\", \"env_to\", \"acc\", \"description\"]\n",
    "\n",
    "# All_Hits_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# # Loop over the HMM DOMTBLOUT files and filter the results based on bit score, e-value and coverage\n",
    "# # hmm name and bit score are in the Actino_BitScore_Criteria series\n",
    "# for hmmsearch, threshold in Actino_BitScore_Criteria.items():\n",
    "#     # Load the \"domtblout\" file into a pandas DataFrame\n",
    "#     file_path = os.path.join(domtblout_dir, hmmsearch + \".domtblout\")\n",
    "#     df = pd.read_csv(file_path, comment=\"#\", sep='\\s+', header=None)    \n",
    "#     # Assign column names to the DataFrame\n",
    "#     df.columns = [\"target_name\", \"accession\", \"tlen\", \"query_name\", \"accession2\", \"qlen\", \"E-value\", \"score\", \"bias\",\n",
    "#                   \"num_domains_index\", \"num_domains_total\", \"c-Evalue\", \"i-Evalue\", \"score2\", \"bias2\", \"hmm_from\", \"hmm_to\", \"ali_from\", \"ali_to\",\n",
    "#                   \"env_from\", \"env_to\", \"acc\", \"description\"]\n",
    "\n",
    "#     # Calculate the coverage for each hit\n",
    "#     df[\"coverage\"] = (df[\"ali_to\"] - df[\"ali_from\"] + 1) / df[\"tlen\"]\n",
    "    \n",
    "#     # Filter the DataFrame by E-value, coverage, and bit-score\n",
    "#     significant_hits = df[(df[\"E-value\"] <= 0.001) & (df[\"coverage\"] > 0.50) & (df[\"score\"] > threshold)]\n",
    "\n",
    "#     # Extract Target nmae and store it in the dictionary\n",
    "#     if not significant_hits.empty:\n",
    "#         MAGs_Hits_name = significant_hits[\"target_name\"].tolist()\n",
    "#         MAGs_Hits[hmmsearch] = MAGs_Hits_name\n",
    "#     else:\n",
    "#         MAGs_Hits_name = None\n",
    "#         MAGs_Hits[hmmsearch] = MAGs_Hits_name\n",
    "\n",
    "# a = 0        \n",
    "# for key in MAGs_Hits:\n",
    "#     a += len(MAGs_Hits[key])\n",
    "    \n",
    "# # a 25082"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f092299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install the pandas package using pip\n",
    "# !pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e7385246",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # The goal is to add the Biomes data into the metagenome table.\n",
    "# # pause the shell. It fails\n",
    "\n",
    "# # Import the pandas package\n",
    "# import pandas as pd\n",
    "\n",
    "# # Load the TSV data into a DataFrame\n",
    "# MetaDf = pd.read_csv('..\\data\\external\\Paper_genome_metadata.tsv', delimiter='\\t', header=0)\n",
    "# # -print(MetaDf.head())\n",
    "\n",
    "# # Load a Dataframe with the lookup values\n",
    "# BioToEco = pd.read_csv('..\\data\\interim\\BiomesToSub_ecosystem.csv')\n",
    "# # -print(BioToEco.head())\n",
    "\n",
    "# # To prepare for merging Biomes data with MetaDf, extract the relevant column to be used. Keys are ecosystem data.\n",
    "\n",
    "# Bio_Eco1 = BioToEco.loc[:,['Biomes', 'ecosystem_category']]\n",
    "\n",
    "# Bio_Eco2 = BioToEco.loc[:,['Biomes', 'ecosystem_type']]\n",
    "\n",
    "# Bio_Eco3 = BioToEco.loc[:,['Biomes', 'habitat']]\n",
    "\n",
    "# # use merge() function to join the Biomes data\n",
    "\n",
    "# MetaDf_Bio_Category = pd.merge(MetaDf, Bio_Eco1, on='ecosystem_category', how='left')\n",
    "\n",
    "# MetaDf_Bio_type = pd.merge(MetaDf_Bio_Category, Bio_Eco2, on='ecosystem_type', how='left')\n",
    "\n",
    "# MetaDf_Bio_habitat = pd.merge(MetaDf_Bio_type, Bio_Eco3, on='habitat', how='right')\n",
    "# print(MetaDf_Bio_habitat)\n",
    "# # MetaDf_Bio = pd.merge(pd.merge(pd.merge(MetaDf_Bio_Ecosystem, MetaDf_Bio_Category, on='genome_id'), MetaDf_Bio_type, on='genome_id'), MetaDf_Bio_habitat, on='genome_id')\n",
    "# # MetaDf_Bio\n",
    "\n",
    "# # # MetaDf_Bio_columns = MetaDf_Bio.columns\n",
    "\n",
    "# # # # Change the column order\n",
    "# # # -MetaDf_Bio_columns\n",
    "# # # MetaDf_Bio = MetaDf_Bio[['genome_id', 'metagenome_id', 'genome_length', 'num_contigs', 'n50',\n",
    "# # #        'num_16s', 'num_5s', 'num_23s', 'num_trna', 'completeness',\n",
    "# # #        'contamination', 'quality_score', 'mimag_quality', 'otu_id',\n",
    "# # #        'ecosystem', 'Biomes', 'ecosystem_category', 'ecosystem_type', 'habitat',\n",
    "# # #        'longitude', 'latitude']]\n",
    "# # # # -MetaDf_Bio\n",
    "\n",
    "# # # MetaDf_Bio.to_csv('..\\data\\processed\\Table\\MetagenomeWithBiomes.csv', index=False)\n",
    "# # # # 確認過後MetaDf_Bio還須修改，因為要分開merge，接續merge的話資料只會呈現在ecosystem有隊到 key 的資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a315c0c-ee20-4073-bbba-f4e5fb361a26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
